{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "000c47a8",
   "metadata": {},
   "source": [
    "# 2024-04-11\n",
    "## Python Study for PBI Lab Crew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a6cb1",
   "metadata": {},
   "source": [
    "# Previous our study\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d85895f",
   "metadata": {},
   "source": [
    "## \"주어진\" 데이터로부터 \"스스로\" 파라미터(웨이트 등)을 수정해가며 더 나은 성능을 가지는 모델이 되어간다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827352c6",
   "metadata": {},
   "source": [
    "## 오늘 주제는 \"어떠한 기준\"으로,  \"어떻게\" 모델이 학습되는가 입니다.\n",
    "### regression을 예시로 할게요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751bd1dc",
   "metadata": {},
   "source": [
    "# 1. 어떠한 기준으로 학습되는가 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b59d8a",
   "metadata": {},
   "source": [
    "![img](https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-08-24-linear_regression/pic7.png)\n",
    "![img](https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-08-24-linear_regression/pic8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d4619",
   "metadata": {},
   "source": [
    "## 우리는 여기서 어떤 직선이 우리의 데이터를 가장 잘 설명해주는 모델인지 결정할 수 있어야 한다.\n",
    "\n",
    "## 여기서 ‘가장 잘 설명한다’는 말을 다른 말로 정의하자면 모델과 데이터 간의 격차가 가장 적어야 한다고 말할 수도 있을 것 같다.\n",
    "\n",
    "## 다시 말해 전체 데이터에 대해 평균적으로 오차(error)가 가장 작은 모델이 더 좋은 모델이라고 말할 수 있다.\n",
    "https://angeloyeo.github.io/2020/08/24/linear_regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4701066b",
   "metadata": {},
   "source": [
    "![img](https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-08-24-linear_regression/pic9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24660d84",
   "metadata": {},
   "source": [
    "## 1) MSE\n",
    "### 평균제곱오차(Mean Squared Error, MSE)는 이름에서 알 수 있듯이 오차(error)를 제곱한 값의 평균입니다. <br>알고리즘이 정답을 잘 맞출수록 MSE 값은 작겠죠. <br>MSE 값은 작을수록 알고리즘의 성능이 좋다고 볼 수 있습니다. 수식을 살펴보겠습니다.\n",
    "![img](MSE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc33d18",
   "metadata": {},
   "source": [
    "### 2) MAE \n",
    "### 평균절대오차(Mean Absolute Error, MAE)는 이름에서 알 수 있듯이 오차(error)의 절대값입니다. <br>알고리즘이 정답을 잘 맞출수록 MAE 값은 작겠죠. <br>MAE 값은 작을수록 알고리즘의 성능이 좋다고 볼 수 있습니다. 수식을 살펴보겠습니다.\n",
    "![img](MAE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff57030",
   "metadata": {},
   "source": [
    "## MSE vs MAE\n",
    "\n",
    "간단하게 정리하자면 \n",
    "MSE는 오차에 대하여 민감하게 반응할 수 있고, MAE는 오차에 덜 민감한 만큼 이상치에 대하여 견고한 모델을 만들 수 있다.\n",
    "각각 상황에 맞게 선택하자 <br>\n",
    "\n",
    "MSE<br>\n",
    "https://heytech.tistory.com/362\n",
    "\n",
    "MAE<br>\n",
    "https://heytech.tistory.com/379\n",
    "\n",
    "L1 norm vs L2 norm<br>\n",
    "https://hyungbinklm.tistory.com/39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c744375",
   "metadata": {},
   "source": [
    "## 이처럼 학습에 기준이 되는 함수를 Objective Function, 혹은 목표와 차이라는 의미에서 Loss Function(손실함수)이라 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d83d8",
   "metadata": {},
   "source": [
    "Object Function은 각각의 Task에 맞게 정의되며 각 상황에 알맞게 사용해야합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e74a1",
   "metadata": {},
   "source": [
    "# 2. 어떻게 학습되는가 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d73a849",
   "metadata": {},
   "source": [
    "## 즉 학습이라는 과정은 파라미터를 수정하며 Loss를 최소화 해 가는 과정을 의미한다고 할 수 있습니다.\n",
    "## 이러한 문제를 해결하기 위해 수식으로 나타내면, Objective Function을 L, L의 파라미터를 w라 할때 L을 최소화 하는 w^를 찾는 식으로 나타낼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae4df4b",
   "metadata": {},
   "source": [
    "![img](argminL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e08ee5",
   "metadata": {},
   "source": [
    "### 즉 함수를 찾는 문제를 L에 대한 최솟값을 구하는 문제로 간소화 할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fdbd95",
   "metadata": {},
   "source": [
    "# Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf1c08",
   "metadata": {},
   "source": [
    "위에서 간소화한 문제를 해결하는 데 사용하는 방법이 경사하강법(Gradient Descent)입니다.\n",
    "\n",
    "함수 L위에 임의의 한점에서 함수 L에 대한 접선의 기울기를 이용하여 현재 임의의 한점을 어디로 이동시킬 지 방향을 결정하게 됩니다. \n",
    "\n",
    " 예를 들어, 아래와 같은 이차함수 형태가 있다고 할 때, 기울기 > 0 이면, x축의 값을 작아지느 쪽으로 이동하고, 기울기 < 0 이면, x축의 값을 커지는 쪽으로 이동하게 됩니다. \n",
    " 위 과정을 반복하면서 최소값을 탐색하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99782182",
   "metadata": {},
   "source": [
    "![img](https://editor.analyticsvidhya.com/uploads/631731_P7z2BKhd0R-9uyn9ThDasA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3571d2c6",
   "metadata": {},
   "source": [
    "![img](https://easyai.tech/wp-content/uploads/2019/01/tiduxiajiang-1.png.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76035fb4",
   "metadata": {},
   "source": [
    "![img](GD3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45852a36",
   "metadata": {},
   "source": [
    "그럼 여기서 궁금할 수 있는게 만약에 x축으로 이동하는 크기가 너무 크다면 탐색은 최소값으로 수렴하지 않고 발산할 수도 있다는 점입니다.\n",
    " \n",
    " 또 Loss Function이 다차원이라면 탐색의 방향을 설정하는 것 또한 문제가 될 수 있습니다.\n",
    " \n",
    " 이때 이동하는 크기를 Learning rate라고도 합니다.\n",
    " \n",
    " 이러한 문제들을 해결하기 위해 SGD, Momentum, RMSprop, Adam 들과 같이 발전해왔고 이런 종류의 알고리즘들을 최적화 알고리즘, Optimizer라고 합니다.\n",
    " \n",
    " 가장 일반적으로 사용되는 최적화 알고리즘은 Adam입니다.\n",
    "\n",
    " ![img](https://velog.velcdn.com/images/freesky/post/57e14895-6eb0-4c86-a9d1-0acdb0398406/image.png)\n",
    " \n",
    " 더 궁금하시면 아래 블로그를 참조해주세요\n",
    "  \n",
    " 간단한 서머리<br>\n",
    " https://velog.io/@freesky/Optimizer\n",
    " \n",
    " 조금 더 궁금하다면?<br>\n",
    " https://velog.io/@yookyungkho/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%98%B5%ED%8B%B0%EB%A7%88%EC%9D%B4%EC%A0%80-%EC%A0%95%EB%B3%B5%EA%B8%B0%EB%B6%80%EC%A0%9C-CS231n-Lecture7-Review"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4caf4e8",
   "metadata": {},
   "source": [
    "### 우리가 오늘 배울 내용에서는 LossFunction과 Optimizer를 직접 다루지 않습니다.\n",
    " 모델이 학습하는 과정을 직관적으로 표현하기 위해서 MSE와 GD를 예시로 가져온 것입니다.\n",
    " 오늘 해볼 Regression task에서 사용하는 알고리즘은 Ordinary Least Squares (OLS) 과  coordinate descent algorithm(CDA)입니다. \n",
    " \n",
    " ![img](RSS.png)\n",
    " \n",
    " https://teddylee777.github.io/scikit-learn/linear-regression/\n",
    " \n",
    " https://mn0317.tistory.com/10\n",
    "\n",
    "\n",
    "단 다음 혹은 다다음 차수에서 배울 perceptron과 답러닝 부분에서는 매우 중요한 파라미터입니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7305cac",
   "metadata": {},
   "source": [
    "물론 MSE와 GD를 이용해서 모델링을 할 수 도 있지만, \n",
    "library에서 제공하는 함수를 사용하는 편이 여러모로 편안합니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee2366",
   "metadata": {},
   "source": [
    "## 뭐시 중헌디 \n",
    "\n",
    "# 데이터를 일반적으로 가장 잘 대표하는 모델을 찾는 것!\n",
    "\n",
    "### 모델을 확습하는 데 사용할 수 있는 방법은 매우 다양합니다.  \n",
    "#### 한 가지 방법이 모든 것을 해결할 수 없다는 것을 꼭 명심하세요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3364bc",
   "metadata": {},
   "source": [
    "## 예시를 풀면서 해볼까요??\n",
    "\n",
    "### 수학 이야기가 나온 김에 문제도 수학적으로 한번 접근해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeee66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리를 가져옵니다.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b755e00f",
   "metadata": {},
   "source": [
    "## 간단하게 y = ax + b 함수를 구현해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334b1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = ax + b 여기서는 y=10x+3 인 일차방정식입니다.\n",
    "def taget_function(input_x):\n",
    "    coefficient_a = 10\n",
    "    b = 3\n",
    "    results = coefficient_a * input_x + b \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85742eaa",
   "metadata": {},
   "source": [
    "### 우리가 목표하는 함수를 추정하기 위하여 데이터를 생산해보도록 하겠습니다.\n",
    "0에서 100 사이 무작위로 추출된 20개 정수를 X, <br>\n",
    "target function의 결과를 f(x), <br>\n",
    "학습의 난이도를 위한 노이즈 c라 할 때<br>\n",
    "\n",
    "y=f(x)+c 의 결과를 이용하여 f(x)를 추정해보겠습니다. \n",
    "\n",
    "이때 노이즈 c는 N(0, 10)의 분포를 따릅니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc18a00f",
   "metadata": {},
   "source": [
    "다시 말해서, 모르는 함수 f(x)를 x와 f(x)+c를 이용하여 추정하는 것 입니다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3bb3c",
   "metadata": {},
   "source": [
    "### 1. 데이터 생산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c43d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # 모델이 학습할 input, x입니다.\n",
    "Y = [] # 모델이 학습할 output, y입니다.\n",
    "\n",
    "fxs = []\n",
    "\n",
    "while (len(X) < 20):\n",
    "    x = random.randint(0, 100)\n",
    "    fx = taget_function(x)\n",
    "    c =  np.random.normal(0, 10)\n",
    "    y = fx + c\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "    fxs.append(fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff1eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "plt.scatter(X, Y, color=\"black\") # 학습할 데이터 \n",
    "plt.plot(X, fxs, color=\"blue\", linewidth=1) # 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ed559",
   "metadata": {},
   "source": [
    "### 2. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8640ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "#reshape input X for model\n",
    "X = np.array(X).reshape(-1,1)\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe1627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7ada3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "plt.scatter(X, Y, color=\"black\") # 학습할 데이터 \n",
    "plt.plot(X, regr.predict(X), color=\"Red\", linewidth=1) # 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ecfac4",
   "metadata": {},
   "source": [
    "### 3.Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da603c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_X = [] # 모델을 평가할 input, x입니다.\n",
    "val_Y= [] # 모델을 평가할 output, y입니다.\n",
    "\n",
    "val_fxs = []\n",
    "\n",
    "while (len(val_X) < 10):\n",
    "    x = random.randint(0, 100)\n",
    "    fx = taget_function(x)\n",
    "    c =  np.random.normal(0, 10)\n",
    "    y = fx + c\n",
    "    val_X.append(x)\n",
    "    val_Y.append(y)\n",
    "    val_fxs.append(fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2988f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape input X for model\n",
    "val_X = np.array(val_X).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c64e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "plt.scatter(val_X, val_Y, color=\"black\") # 학습할 데이터 \n",
    "plt.plot(val_X, val_fxs, color=\"blue\", linewidth=2) # 목표\n",
    "plt.plot(val_X, regr.predict(val_X), color=\"Red\", linewidth=1) # 목표\n",
    "\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(val_fxs, regr.predict(val_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd65d4e",
   "metadata": {},
   "source": [
    "한번 학습된 모델의 계수를 확인해볼까요???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aac558",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d618d3",
   "metadata": {},
   "source": [
    "우리가 아까 설정한 계수와 거의 유사한 결과를 얻을 수 있었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87771fe3",
   "metadata": {},
   "source": [
    "### 심화과정 : 더 복잡한 함수를 추정해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837fb684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = (x-1)(x-2)(x-3)\n",
    "def polynomial_taget_function(input_x):\n",
    "    coefficient_a = 1\n",
    "    coefficient_b = -6\n",
    "    coefficient_c = 11\n",
    "    d = -6\n",
    "    results = coefficient_a * input_x * input_x * input_x + coefficient_b * input_x * input_x + coefficient_c * input_x + d\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee17f8f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = [] # 모델이 학습할 input, x입니다.\n",
    "Y = [] # 모델이 학습할 output, y입니다.\n",
    "fxs = []\n",
    "\n",
    "# 0에서 4사이의 임의의 실수를 생산합니다.\n",
    "while (len(X) < 200):\n",
    "    x = random.random() * 4\n",
    "    fx = polynomial_taget_function(x)\n",
    "    c =  np.random.normal(0, 5)\n",
    "    y = fx + c\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "    fxs.append(fx)\n",
    "\n",
    "    \n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "regr_2 = linear_model.LinearRegression()\n",
    "regr_3 = linear_model.LinearRegression()\n",
    "regr_4 = linear_model.LinearRegression()\n",
    "regr_5 = linear_model.LinearRegression()\n",
    "regr_6 = linear_model.LinearRegression()\n",
    "\n",
    "\n",
    "\n",
    "#reshape input X for model\n",
    "X = np.array(X).reshape(-1,1)\n",
    "\n",
    "\n",
    "# 2차 다항식 변환\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "train_pf_2 = PolynomialFeatures(degree=2)\n",
    "X_train_2_poly = train_pf_2.fit_transform(X)\n",
    "\n",
    "train_pf_3 = PolynomialFeatures(degree=3)\n",
    "X_train_3_poly = train_pf_3.fit_transform(X)\n",
    "\n",
    "train_pf_4 = PolynomialFeatures(degree=4)\n",
    "X_train_4_poly = train_pf_4.fit_transform(X)\n",
    "\n",
    "train_pf_5 = PolynomialFeatures(degree=5)\n",
    "X_train_5_poly = train_pf_5.fit_transform(X)\n",
    "\n",
    "train_pf_6 = PolynomialFeatures(degree=6)\n",
    "X_train_6_poly = train_pf_6.fit_transform(X)\n",
    "\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X, Y)\n",
    "regr_2.fit(X_train_2_poly, Y)\n",
    "regr_3.fit(X_train_3_poly, Y)\n",
    "regr_4.fit(X_train_4_poly, Y)\n",
    "regr_5.fit(X_train_5_poly, Y)\n",
    "regr_6.fit(X_train_6_poly, Y)\n",
    "\n",
    "\n",
    "val_X = [] # 모델을 평가할 input, x입니다.\n",
    "val_Y= [] # 모델을 평가할 output, y입니다.\n",
    "val_fxs = []\n",
    "\n",
    "\n",
    "\n",
    "while (len(val_X) < 10):\n",
    "    x = random.random() * 4\n",
    "    fx = polynomial_taget_function(x)\n",
    "    c =  np.random.normal(0, 5)\n",
    "    y = fx + c\n",
    "    val_X.append(x)\n",
    "    val_Y.append(y)\n",
    "    val_fxs.append(fx)\n",
    "\n",
    "# for visualization \n",
    "val_X_sort = np.sort(val_X,axis=0)\n",
    "\n",
    "\n",
    "#reshape input X for model\n",
    "val_X = np.array(val_X).reshape(-1,1)\n",
    "\n",
    "val_pf_2 = PolynomialFeatures(degree=2)\n",
    "X_val_2_poly = val_pf_2.fit_transform(val_X)\n",
    "val_pf_3 = PolynomialFeatures(degree=3)\n",
    "X_val_3_poly = val_pf_3.fit_transform(val_X)\n",
    "val_pf_4 = PolynomialFeatures(degree=4)\n",
    "X_val_4_poly = val_pf_4.fit_transform(val_X)\n",
    "val_pf_5 = PolynomialFeatures(degree=5)\n",
    "X_val_5_poly = val_pf_5.fit_transform(val_X)\n",
    "val_pf_6 = PolynomialFeatures(degree=6)\n",
    "X_val_6_poly = val_pf_6.fit_transform(val_X)\n",
    "\n",
    "\n",
    "\n",
    "# Plot outputs\n",
    "\n",
    "val_X_sort = np.sort(val_X,axis=0)\n",
    "\n",
    "print(\"Degree 1 Model\")\n",
    "print(\"Degree 1 Model mean squared error: %.2f\" % mean_squared_error(val_fxs, regr.predict(val_X)))\n",
    "print(\"Degree 1 Model R^2: %.2f\" % regr.score(val_X,val_fxs))\n",
    "print(\"Degree 1 Model Coeffcients : \",end=\"\")\n",
    "print(regr.coef_)\n",
    "print(\"Degree 1 Model intercept : \",end=\"\")\n",
    "print(regr.intercept_)\n",
    "\n",
    "plt.scatter(val_X, val_fxs, color=\"blue\", linewidth=2) # 목표\n",
    "plt.plot(val_X_sort, regr.predict(val_X_sort), color=\"Red\", linewidth=1) \n",
    "plt.show()\n",
    "\n",
    "print(\"Degree 2 Model\")\n",
    "print(\"Degree 2 Model mean squared error: %.2f\" % mean_squared_error(val_fxs, regr_2.predict(X_val_2_poly)))\n",
    "print(\"Degree 2 Model R^2: %.2f\" % regr_2.score(X_val_2_poly,val_fxs))\n",
    "print(\"Degree 2 Model Coeffcients : \",end=\"\")\n",
    "print(regr_2.coef_)\n",
    "print(\"Degree 2 Model intercept : \",end=\"\")\n",
    "print(regr_2.intercept_)\n",
    "      \n",
    "plt.scatter(val_X, val_fxs, color=\"blue\", linewidth=2) # 목표\n",
    "plt.plot(val_X_sort, regr_2.predict(val_pf_2.fit_transform(val_X_sort)), color=\"Red\", linewidth=1) \n",
    "plt.show()\n",
    "\n",
    "print(\"Degree 3 Model\")\n",
    "print(\"Degree 3 Model mean squared error: %.2f\" % mean_squared_error(val_fxs, regr_3.predict(X_val_3_poly)))\n",
    "print(\"Degree 3 Model R^2: %.2f\" % regr_3.score(X_val_3_poly,val_fxs))\n",
    "print(\"Degree 3 Model Coeffcients : \",end=\"\")\n",
    "print(regr_3.coef_)\n",
    "print(\"Degree 2 Model intercept : \",end=\"\")\n",
    "print(regr_3.intercept_)\n",
    "      \n",
    "plt.scatter(val_X, val_fxs, color=\"blue\", linewidth=2) # 목표\n",
    "plt.plot(val_X_sort, regr_3.predict(val_pf_3.fit_transform(val_X_sort)), color=\"Red\", linewidth=1) \n",
    "plt.show()\n",
    "\n",
    "print(\"Degree 4 Model\")\n",
    "print(\"Degree 4 Model mean squared error: %.2f\" % mean_squared_error(val_fxs, regr_4.predict(X_val_4_poly)))\n",
    "print(\"Degree 4 Model R^2: %.2f\" % regr_4.score(X_val_4_poly,val_fxs))\n",
    "print(\"Degree 4 Model Coeffcients : \",end=\"\")\n",
    "print(regr_4.coef_)\n",
    "print(\"Degree 4 Model intercept : \",end=\"\")\n",
    "print(regr_4.intercept_)\n",
    "      \n",
    "plt.scatter(val_X, val_fxs, color=\"blue\", linewidth=2) # 목표\n",
    "plt.plot(val_X_sort, regr_4.predict(val_pf_4.fit_transform(val_X_sort)), color=\"Red\", linewidth=1) \n",
    "plt.show()\n",
    "\n",
    "print(\"Degree 5 Model\")\n",
    "print(\"Degree 5 Model mean squared error: %.2f\" % mean_squared_error(val_fxs, regr_5.predict(X_val_5_poly)))\n",
    "print(\"Degree 5 Model R^2: %.2f\" % regr_5.score(X_val_5_poly,val_fxs))\n",
    "print(\"Degree 5 Model Coeffcients : \",end=\"\")\n",
    "print(regr_5.coef_)\n",
    "print(\"Degree 5 Model intercept : \",end=\"\")\n",
    "print(regr_5.intercept_)\n",
    "      \n",
    "plt.scatter(val_X, val_fxs, color=\"blue\", linewidth=2) # 목표\n",
    "plt.plot(val_X_sort, regr_5.predict(val_pf_5.fit_transform(val_X_sort)), color=\"Red\", linewidth=1) \n",
    "plt.show()\n",
    "\n",
    "print(\"Degree 6 Model\")\n",
    "print(\"Degree 6 Model mean squared error: %.2f\" % mean_squared_error(val_fxs, regr_6.predict(X_val_6_poly)))\n",
    "print(\"Degree 6 Model R^2: %.2f\" % regr_6.score(X_val_6_poly,val_fxs))\n",
    "print(\"Degree 6 Model Coeffcients : \",end=\"\")\n",
    "print(regr_6.coef_)\n",
    "print(\"Degree 6 Model intercept : \",end=\"\")\n",
    "print(regr_6.intercept_)\n",
    "      \n",
    "plt.scatter(val_X, val_fxs, color=\"blue\", linewidth=2) # 목표\n",
    "plt.plot(val_X_sort, regr_6.predict(val_pf_6.fit_transform(val_X_sort)), color=\"Red\", linewidth=1) \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다면량 함수의 경우\n",
    "def multivariate_taget_function(input_x1, input_x2, input_x3):\n",
    "    coefficient_a = 6\n",
    "    coefficient_b = 4\n",
    "    coefficient_c = 10\n",
    "    d = 1\n",
    "    results = coefficient_a * input_x1 + coefficient_b * input_x2 + coefficient_c * input_x3  + d\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = [] # 모델이 학습할 input, x1입니다.\n",
    "X2 = [] # 모델이 학습할 input, x2입니다.\n",
    "X3 = [] # 모델이 학습할 input, x3입니다.\n",
    "Y = [] # 모델이 학습할 output, y입니다.\n",
    "fxs = []\n",
    "\n",
    "while (len(fxs) < 200):\n",
    "    x1 = random.randint(0, 10)\n",
    "    x2 = random.randint(0, 10)\n",
    "    x3 = random.randint(0, 10)\n",
    "    \n",
    "    fx = multivariate_taget_function(x1,x2,x3)\n",
    "    c =  np.random.normal(0, 10)\n",
    "    y = fx + c\n",
    "    \n",
    "    X1.append(x1)\n",
    "    X2.append(x2)\n",
    "    X3.append(x3)\n",
    "    \n",
    "    Y.append(y)\n",
    "    fxs.append(fx)\n",
    "\n",
    "    \n",
    "# Create linear regression object\n",
    "multivariate_regr = linear_model.LinearRegression()\n",
    "\n",
    "\n",
    "#reshape input X for model\n",
    "X = np.array([X1,X2,X3]).T\n",
    "\n",
    "\n",
    "val_X1 = [] # 모델이 학습할 input, x1입니다.\n",
    "val_X2 = [] # 모델이 학습할 input, x2입니다.\n",
    "val_X3 = [] # 모델이 학습할 input, x3입니다.\n",
    "val_Y = [] # 모델이 학습할 output, y입니다.\n",
    "val_fxs = []\n",
    "\n",
    "while (len(val_fxs) < 10):\n",
    "    x1 = random.randint(0, 10)\n",
    "    x2 = random.randint(0, 10)\n",
    "    x3 = random.randint(0, 10)\n",
    "    \n",
    "    fx = multivariate_taget_function(x1,x2,x3)\n",
    "    c =  np.random.normal(0, 10)\n",
    "    y = fx + c\n",
    "    \n",
    "    val_X1.append(x1)\n",
    "    val_X2.append(x2)\n",
    "    val_X3.append(x3)\n",
    "    \n",
    "    val_Y.append(y)\n",
    "    val_fxs.append(fx)\n",
    "    \n",
    "X_val = np.array([val_X1,val_X2,val_X3]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ddf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_regr.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0042d98e",
   "metadata": {},
   "source": [
    "### 다변량함수는 시각화하기 어려우니 결과만 확인하고 넘어가겠습니다.~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a13f7ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Degree 5 Model mean squared error: %.2f\" % mean_squared_error(val_fxs, multivariate_regr.predict(X_val)))\n",
    "print(\"Degree 5 Model R^2: %.2f\" % multivariate_regr.score(X_val,val_fxs))\n",
    "print(\"Degree 5 Model Coeffcients : \",end=\"\")\n",
    "print(multivariate_regr.coef_)\n",
    "print(\"Degree 5 Model intercept : \",end=\"\")\n",
    "print(multivariate_regr.intercept_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4872a7ff",
   "metadata": {},
   "source": [
    "### 간단한 Linear Regression 을 \"체험\" 해보았습니다. \n",
    "\n",
    "###  방금 같은 예시는 우리가 데이터의 형태(추정하고 싶은 모집단)를 어느 정도 알고 있었기에 현재 모델이 Overfitting 인지 Underfitting인지 파악할 수 있었고, 또 입력형태(like Degree)를 알고 있었기에  모델에 맞게 넣어줄 수 있었습니다.\n",
    "\n",
    "###  실제 데이터를 다룰 경우에는 모델이 어떻게 변화할 지 예측하는 게 쉽지 않고 그렇기에 쉽게 Overfitting 될 수 있습니다. <br> 이러한 경우를 막기위해 제한 조건(regularizer)을 사용합니다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598787e7",
   "metadata": {},
   "source": [
    "### 지금부터 해볼 LASSO는 Regression에 regularizer를 적용하고 또 그 regularization의 특징을 이용할 수 있어서 널리 사용되는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e18a96a",
   "metadata": {},
   "source": [
    "## 왜 LASSO 일까"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d865a",
   "metadata": {},
   "source": [
    "# LASSO(least absolute shrinkage and selection operator) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb09b7",
   "metadata": {},
   "source": [
    "![img](LASSO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998dcc13",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Lasso_(statistics) <br>\n",
    "https://otugi.tistory.com/127 <br>\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04a3fa7",
   "metadata": {},
   "source": [
    "이름에서도 느껴지는 것처럼 모델의 파라미터들(θ)의 절대값의 합을 수축(제한)하여 모델을 학습하는 방법입니다. \n",
    "우리는 이를 통해서 일부 파라미터들의 값이 0으로 변화하는 것을 관측할 수 있고, 이를 통해서 특정 Feature를 선택하는데 응용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c7e0e9",
   "metadata": {},
   "source": [
    "개인 선택 학습\n",
    "\n",
    "왜 LASSO에서 파라미터를 0으로 만드는 게 절대값을 이용하기에 가능한 것인지,\n",
    "\n",
    "또 다른 제한 조건이 있는 모델과 비교를 했을때는 어떤 차이를 보내는지 궁금하다면\n",
    "\n",
    "Lasso and Ridge Regression in Python Tutorial\n",
    "https://www.datacamp.com/tutorial/tutorial-lasso-ridge-regression\n",
    "\n",
    "https://velog.io/@jochedda/%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D%EB%9D%BC%EC%8F%98-%ED%9A%8C%EA%B7%80Lasso-Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8713e3b",
   "metadata": {},
   "source": [
    "## 위에 보이는 수식에서의 알파는 모델에서 정해지는 파라미터가 아닌 사용자가 설정을 해주어야 하는 Hyper Parameter입니다. \n",
    "## 알파를 이용해서 L1-norm penalty와 MSE 사이에 반영비율을 설정합니다.  \n",
    "## 따라서 LASSO를 이용할 때는 적절한 Alpha 를 설정하는 것 부터 시작해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb025f4",
   "metadata": {},
   "source": [
    "## 생각해보기\n",
    "### 변화하는 Alpha의 값을생각해보자 만약 Alpha 값이 적절한 값보다 높다면 모델의 결과는 어떻게 변화할까??\n",
    "### 만약 낮다면 어떻게 변할까???\n",
    "\n",
    "### 생각을 아래 빈 셀에 작성해주시고 저를 불러주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa1b7b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3908e32b",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">실습시 주의사항 - 과몰입 금지</span>\n",
    "## 실습문제는 실제 제 데이터 중 일부이긴하나\n",
    "## 편안한 실습을 위해 편의로 가공한 내용입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DE_results = pd.read_csv(\"genes.counts.matrix.Blackblight_vs_Fireblight.edgeR.DE_results\",sep=\"\\t\")\n",
    "tmm = pd.read_csv(\"kyuchan.TMM.matrix\",sep = \"\\t\", index_col=0)\n",
    "\n",
    "tmm = tmm[['N10-na1', 'N10-na2', 'N10-na3', 'N10-na4', 'N10-na5', 'N11-na1',\n",
    "       'N11-na2', 'N11-na3', 'N11-na4', 'N11-na5', 'P10-dh1', 'P10-dh2',\n",
    "       'P10-dh3', 'P10-dh4', 'P10-dh5', 'P10-ur1', 'P10-ur2', 'P10-ur3',\n",
    "       'P10-ur4', 'P10-ur5', 'P11-an1', 'P11-an2', 'P11-an3', 'P11-an4',\n",
    "       'P11-an5', 'P11-as1', 'P11-as2', 'P11-as3', 'P11-as4', 'P11-as5']]\n",
    "samples = pd.DataFrame()\n",
    "samples[\"Sample\"] = [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "\n",
    "gene_list = DE_results[DE_results.FDR < 0.0001].index \n",
    "matrix = tmm.loc[gene_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da85fe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf30a9",
   "metadata": {},
   "source": [
    "# binomial deviance 이항 편차가 궁금하다면 -> https://kuklife.tistory.com/49\n",
    "낮을 수록 높은 성능의 모델이라고 생각해주시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1754ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "# alpha는 0이 되면 LASSO가 오류를 배출합니다. 아는 데 그림 그릴 때 0값이 있는 게 이뻐서 \n",
    "alphas =[0,\n",
    "0.0000001,0.0000002,0.0000003,0.0000004,0.0000005,0.0000006,0.0000007,0.0000008,0.0000009,        \n",
    "0.000001,0.000002,0.000003,0.000004,0.000005,0.000006,0.000007,0.000008,0.000009,\n",
    "0.00001,0.00002,0.00003,0.00004,0.00005,0.00006,0.00007,0.00008,0.00009,\n",
    "0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0008,0.0009,\n",
    "0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,\n",
    "0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,\n",
    "0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,\n",
    "1,2,3,4,5,6,7,8,9,10\n",
    "]\n",
    "lasso = Lasso(max_iter=1000000)\n",
    "coefs = []\n",
    "maes = []\n",
    "bd = [] \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "trnx, tstx, trny, tsty = train_test_split(matrix.T, samples.Sample, test_size = 0.2, random_state = 0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(trnx)\n",
    "trnx_scaled = scaler.transform(trnx)\n",
    "tstx_scaled = scaler.transform(tstx)\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(trnx_scaled, trny)\n",
    "    coefs.append(lasso.coef_)\n",
    "    mae = sum(abs(tsty - list(lasso.predict(tstx_scaled))))/len(tsty)\n",
    "    logistic_probs = 1 / (1 + np.exp(-lasso.predict(tstx_scaled)))\n",
    "    binomial_deviance = log_loss(tsty, logistic_probs)\n",
    "    bd.append(binomial_deviance)                            \n",
    "    maes.append(mae)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fc177",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(alphas, bd, linewidth=4)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('alpha')\n",
    "plt.axvline(0.000008, color='gray', linestyle='--', linewidth=4)\n",
    "plt.ylabel('Binomial Deviance')\n",
    "plt.title('Lasso Mean Abosolute Error as a function of alpha', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 사이즈\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 10]\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.axvline(0.000008, color='gray', linestyle='--', linewidth=4)\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('Lasso coefficients as a function of alpha', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5543638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.000007, max_iter=1000000)\n",
    "clf.fit(matrix.T, samples.Sample)\n",
    "sum(clf.coef_ != 0.00000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1933d6c4",
   "metadata": {},
   "source": [
    "### 이렇게 우리는 179개의 유전자를 162개로 줄였습니다. \n",
    "\n",
    "### 실제 사용시에는 binomial_deviance 그래프에 에러바가 그려저 있는 데, 이는 단일 CV에사용하지 않고 K-fold CV를 해서 범위를 얻었기 때문입니다. 당연히 하나를 보여주는 것보단 여러번 해서 보여주는 게 좋겠죠??\n",
    "\n",
    "### 또한 Coefficeient그림에서 우리가 선택한 위치 이후에 여러값이 한번에 수평선을 그리는 것처럼 보이는데 이는 좋지 못한 현상이며\n",
    "### 대규모 동시 분기가 일어나기 전의 alpha 값을 찾느게 더 이상적인 경우 입니다.\n",
    "### 물론 우리의 alpha 또한 bd값의 피크에서 선발되었기 떄문에 양호한 값입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe549c",
   "metadata": {},
   "source": [
    "# 블로그 & 유튜버 추천 \n",
    "여기 괜찮아요~\n",
    "https://angeloyeo.github.io/2020/08/16/gradient_descent.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
